# doc-slurp

> Ingest all Markdown files from a GitHub organisation, group them per repository, build a [Docusaurus](https://docusaurus.io) site with a **static search index**, and deploy it to **GitHub Pages** — with **incremental scraping** so only changed files are re-fetched.

---

## Features

| Feature | Details |
|---|---|
| **GitHub org scraping** | Fetches every `.md` file from every repo in a GitHub organisation using the GitHub REST API |
| **Per-repo grouping** | Files are organised under `website/docs/<owner/repo>/` so each repo becomes its own sidebar section |
| **Incremental scraping** | Git blob SHAs are cached in `scrape-state.json`; files with unchanged SHAs are skipped to save API quota |
| **Rate-limit handling** | Uses `@octokit/plugin-throttling` for automatic retry/back-off on GitHub rate-limit responses |
| **Static search index** | `@easyops-cn/docusaurus-search-local` generates a fully-offline, client-side search index at build time |
| **GitHub Pages deployment** | A scheduled (or manually-triggered) GitHub Actions workflow scrapes → builds → deploys |
| **React + TypeScript UI** | A Vite-powered admin app lets you interactively configure the org, enter a token, and inspect scrape results in the browser |

---

## Quick start

### Prerequisites

* Node.js ≥ 20
* A GitHub Personal Access Token with `repo:read` scope (optional but recommended to avoid the 60 req/hr unauthenticated limit)

### 1. Install

```bash
npm install
```

### 2. Run the admin UI (browser)

```bash
npm run dev
```

Open `http://localhost:5173`, enter your GitHub org name and (optionally) a token, then click **Start Scrape**.  
Scraped files and the incremental cache state are stored in `localStorage` — subsequent scrapes will skip unchanged files automatically.

### 3. Scrape from the command line (Node.js)

```bash
GITHUB_ORG=my-org GITHUB_TOKEN=ghp_xxx npm run scrape
```

Environment variables:

| Variable | Required | Description |
|---|---|---|
| `GITHUB_ORG` | ✅ | Organisation to scrape |
| `GITHUB_TOKEN` | ☐ | PAT for higher rate limits |
| `REPO_FILTER` | ☐ | Regex pattern — only matching repo names are scraped |
| `DOCS_OUT_DIR` | ☐ | Output directory (default: `website/docs`) |
| `STATE_FILE` | ☐ | Path to incremental state JSON (default: `scrape-state.json`) |

### 4. Build the documentation site

```bash
npm run docs:build
```

This runs `generate-config` (produces `website/docusaurus.config.ts` + `website/sidebars.ts` from the discovered repos) then invokes the Docusaurus build.

### 5. Preview locally

```bash
npm run docs:serve
```

---

## Automated pipeline (GitHub Actions)

Two workflows are included:

### `scrape-and-deploy.yml`

Triggered daily at 02:00 UTC **and** on demand via `workflow_dispatch`.

1. Restores `scrape-state.json` from the Actions cache (incremental diff)
2. Runs `npm run scrape`
3. Saves the updated cache
4. Builds the Docusaurus site
5. Deploys to GitHub Pages

**Required secrets / variables:**

| Name | Type | Description |
|---|---|---|
| `GITHUB_ORG` | Secret **or** Variable | Organisation to scrape |
| `SCRAPE_GITHUB_TOKEN` | Secret | PAT for scraping |
| `SITE_TITLE` | Variable | Navbar title (default: `doc-slurp`) |
| `SITE_URL` | Variable | Production URL |
| `BASE_URL` | Variable | Base path (default: `/<repo-name>/`) |

To enable GitHub Pages in your repo: **Settings → Pages → Source → GitHub Actions**.

### `ci.yml`

Runs on every push/PR: type-check, build, and test.

---

## Architecture

```
doc-slurp/
├── src/                         # React + TypeScript admin UI (Vite)
│   ├── lib/
│   │   ├── types.ts             # Shared TypeScript types
│   │   ├── github.ts            # Octokit wrapper (repo listing, tree walk, file fetch)
│   │   └── scraper.ts           # Incremental scraping logic
│   └── components/
│       ├── OrgScraper.tsx       # Configuration form + scrape trigger
│       ├── ScrapeStatus.tsx     # Live progress display
│       └── RepoList.tsx         # Results grouped by repo
├── scripts/
│   ├── scrape.ts                # CLI: scrape org → website/docs/
│   └── generate-config.ts      # CLI: generate docusaurus.config.ts + sidebars.ts
├── website/                     # Docusaurus site (template)
│   ├── docs/                    # Generated — gitignored, populated by scrape script
│   ├── src/css/custom.css
│   ├── docusaurus.config.ts     # Generated by generate-config script
│   └── sidebars.ts              # Generated by generate-config script
└── .github/workflows/
    ├── scrape-and-deploy.yml    # Scheduled scrape + Pages deploy
    └── ci.yml                   # Lint / build / test on PR
```

---

## Incremental scraping

Every `.md` file discovered in the git tree has a **blob SHA** provided by GitHub at no API cost.  
The scraper compares this SHA against the value stored in `scrape-state.json`:

* **SHA unchanged** → file is skipped (no download, no API call beyond the tree fetch).
* **SHA changed / new file** → file content is fetched and written to disk.

The state file is persisted to the GitHub Actions cache between runs so the incremental benefit carries over across workflow executions.

---

## Development

```bash
npm run dev          # Start Vite dev server
npm run type-check   # TypeScript type-check (all source)
npm test             # Run Vitest unit tests
npm run build        # Production build of the React app
```

